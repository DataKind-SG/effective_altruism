{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import re\n",
    "from string import Template\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MYANMMAR_CHARITIES_URL ='https://www.lrcmyanmar.org/ngo-database/?type1=Community%20Based%20Organization#'\n",
    "MYANMMAR_CSV_DUMP_PATH = './NGO_vietnam.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(text, string_append):\n",
    "    #combine the input text with a string to get a website id\n",
    "    to_combine = text.split()\n",
    "    for ix, item in enumerate(to_combine):\n",
    "        if ix != len(to_combine)-1:\n",
    "            to_combine[ix] = item+'%20'\n",
    "    return  'https://www.lrcmyanmar.org' + string_append +''.join(to_combine)\n",
    "\n",
    "def gen_websitecontainer_to_scrap(container):\n",
    "    #return a list of website to scrap\n",
    "    link_to_scrap= []\n",
    "    for items in container:\n",
    "        #import pdb;pdb.set_trace()\n",
    "        try: \n",
    "            #check if the list is more than one element long\n",
    "            list_iterate = items.text.split(',')\n",
    "            for item in list_iterate:\n",
    "                link_to_scrap.append(convert(item,'/ngo-list/?sector='))\n",
    "        except:\n",
    "            link_to_scrap.append(convert(items.text,'/ngo-list/?sector='))\n",
    "    return link_to_scrap\n",
    "\n",
    "def gen_website_to_scrape(list_of_website_container):\n",
    "    website_list = []\n",
    "    for website_container in list_of_website_container:\n",
    "        request1 = requests.get(website_container)\n",
    "        soup1 = BeautifulSoup(request1.text, 'html.parser')\n",
    "        test_list = soup1.find_all('td')\n",
    "        for item in test_list:\n",
    "            website_list.append(convert(item.text,'/ngo-info/?$name='))\n",
    "    return website_list\n",
    "\n",
    "def scrape_site(site_name):\n",
    "\n",
    "    def convert_to_dataframe (X,y):\n",
    "        #X is the keys,while y is the values\n",
    "        dicte = dict(zip(X,y))\n",
    "        df = pd.DataFrame(dicte,index=[0])\n",
    "        return df\n",
    "\n",
    "    table_index=[]\n",
    "    table_val = []\n",
    "    #import pdb;pdb.set_trace()\n",
    "    request = requests.get(site_name)\n",
    "    soup = BeautifulSoup(request.text, 'html.parser')\n",
    "    temp_table_index = soup.find_all('td',class_ = False)\n",
    "    table_index = [x.text for x  in temp_table_index]\n",
    "    temp_table_val = soup.find_all('td', {'class':'tr-main'})\n",
    "    table_val = [x.text for x in temp_table_val]\n",
    "    return convert_to_dataframe(table_index[:-2],table_val[:-2])\n",
    "\n",
    "def scrape_all (website_list):\n",
    "    for ix, website in enumerate(website_list):\n",
    "        #import pdb;pdb.set_trace()\n",
    "        if ix ==0:\n",
    "            df = scrape_site(website)\n",
    "        else:\n",
    "            df = pd.concat([df,scrape_site(website)])\n",
    "        print('site {} out of {} is completed'.format(ix, len(website_list)))\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def data_clean(df):\n",
    "    #data cleansing\n",
    "    f = df['Organization Name'].isna()\n",
    "    #drop those without organization name\n",
    "    df = df.loc[~f]\n",
    "    rename_col = {\n",
    "        'Organization Name':'Name',\n",
    "        'Extra Information ':'description',\n",
    "        'Website':'website',\n",
    "        'Sectors':'cause_area',\n",
    "        'Contact Address':'address',\n",
    "        'States or Regions ': \"city\",\n",
    "        'Contact Phone':'Contact_number',\n",
    "        'Contact Email':'email',\n",
    "        'Contact person':'contact_person' \n",
    "    }\n",
    "    keep = ['Name', 'description', 'website', 'cause_area', 'address', 'city', 'Contact_number', 'email', 'contact_person']\n",
    "    df = df.rename(columns=rename_col)\n",
    "    df = df[keep]\n",
    "    df = df.replace('',np.nan)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site 0 out of 158 is completed\n",
      "site 1 out of 158 is completed\n",
      "site 2 out of 158 is completed\n",
      "site 3 out of 158 is completed\n",
      "site 4 out of 158 is completed\n",
      "site 5 out of 158 is completed\n",
      "site 6 out of 158 is completed\n",
      "site 7 out of 158 is completed\n",
      "site 8 out of 158 is completed\n",
      "site 9 out of 158 is completed\n",
      "site 10 out of 158 is completed\n",
      "site 11 out of 158 is completed\n",
      "site 12 out of 158 is completed\n",
      "site 13 out of 158 is completed\n",
      "site 14 out of 158 is completed\n",
      "site 15 out of 158 is completed\n",
      "site 16 out of 158 is completed\n",
      "site 17 out of 158 is completed\n",
      "site 18 out of 158 is completed\n",
      "site 19 out of 158 is completed\n",
      "site 20 out of 158 is completed\n",
      "site 21 out of 158 is completed\n",
      "site 22 out of 158 is completed\n",
      "site 23 out of 158 is completed\n",
      "site 24 out of 158 is completed\n",
      "site 25 out of 158 is completed\n",
      "site 26 out of 158 is completed\n",
      "site 27 out of 158 is completed\n",
      "site 28 out of 158 is completed\n",
      "site 29 out of 158 is completed\n",
      "site 30 out of 158 is completed\n",
      "site 31 out of 158 is completed\n",
      "site 32 out of 158 is completed\n",
      "site 33 out of 158 is completed\n",
      "site 34 out of 158 is completed\n",
      "site 35 out of 158 is completed\n",
      "site 36 out of 158 is completed\n",
      "site 37 out of 158 is completed\n",
      "site 38 out of 158 is completed\n",
      "site 39 out of 158 is completed\n",
      "site 40 out of 158 is completed\n",
      "site 41 out of 158 is completed\n",
      "site 42 out of 158 is completed\n",
      "site 43 out of 158 is completed\n",
      "site 44 out of 158 is completed\n",
      "site 45 out of 158 is completed\n",
      "site 46 out of 158 is completed\n",
      "site 47 out of 158 is completed\n",
      "site 48 out of 158 is completed\n",
      "site 49 out of 158 is completed\n",
      "site 50 out of 158 is completed\n",
      "site 51 out of 158 is completed\n",
      "site 52 out of 158 is completed\n",
      "site 53 out of 158 is completed\n",
      "site 54 out of 158 is completed\n",
      "site 55 out of 158 is completed\n",
      "site 56 out of 158 is completed\n",
      "site 57 out of 158 is completed\n",
      "site 58 out of 158 is completed\n",
      "site 59 out of 158 is completed\n",
      "site 60 out of 158 is completed\n",
      "site 61 out of 158 is completed\n",
      "site 62 out of 158 is completed\n",
      "site 63 out of 158 is completed\n",
      "site 64 out of 158 is completed\n",
      "site 65 out of 158 is completed\n",
      "site 66 out of 158 is completed\n",
      "site 67 out of 158 is completed\n",
      "site 68 out of 158 is completed\n",
      "site 69 out of 158 is completed\n",
      "site 70 out of 158 is completed\n",
      "site 71 out of 158 is completed\n",
      "site 72 out of 158 is completed\n",
      "site 73 out of 158 is completed\n",
      "site 74 out of 158 is completed\n",
      "site 75 out of 158 is completed\n",
      "site 76 out of 158 is completed\n",
      "site 77 out of 158 is completed\n",
      "site 78 out of 158 is completed\n",
      "site 79 out of 158 is completed\n",
      "site 80 out of 158 is completed\n",
      "site 81 out of 158 is completed\n",
      "site 82 out of 158 is completed\n",
      "site 83 out of 158 is completed\n",
      "site 84 out of 158 is completed\n",
      "site 85 out of 158 is completed\n",
      "site 86 out of 158 is completed\n",
      "site 87 out of 158 is completed\n",
      "site 88 out of 158 is completed\n",
      "site 89 out of 158 is completed\n",
      "site 90 out of 158 is completed\n",
      "site 91 out of 158 is completed\n",
      "site 92 out of 158 is completed\n",
      "site 93 out of 158 is completed\n",
      "site 94 out of 158 is completed\n",
      "site 95 out of 158 is completed\n",
      "site 96 out of 158 is completed\n",
      "site 97 out of 158 is completed\n",
      "site 98 out of 158 is completed\n",
      "site 99 out of 158 is completed\n",
      "site 100 out of 158 is completed\n",
      "site 101 out of 158 is completed\n",
      "site 102 out of 158 is completed\n",
      "site 103 out of 158 is completed\n",
      "site 104 out of 158 is completed\n",
      "site 105 out of 158 is completed\n",
      "site 106 out of 158 is completed\n",
      "site 107 out of 158 is completed\n",
      "site 108 out of 158 is completed\n",
      "site 109 out of 158 is completed\n",
      "site 110 out of 158 is completed\n",
      "site 111 out of 158 is completed\n",
      "site 112 out of 158 is completed\n",
      "site 113 out of 158 is completed\n",
      "site 114 out of 158 is completed\n",
      "site 115 out of 158 is completed\n",
      "site 116 out of 158 is completed\n",
      "site 117 out of 158 is completed\n",
      "site 118 out of 158 is completed\n",
      "site 119 out of 158 is completed\n",
      "site 120 out of 158 is completed\n",
      "site 121 out of 158 is completed\n",
      "site 122 out of 158 is completed\n",
      "site 123 out of 158 is completed\n",
      "site 124 out of 158 is completed\n",
      "site 125 out of 158 is completed\n",
      "site 126 out of 158 is completed\n",
      "site 127 out of 158 is completed\n",
      "site 128 out of 158 is completed\n",
      "site 129 out of 158 is completed\n",
      "site 130 out of 158 is completed\n",
      "site 131 out of 158 is completed\n",
      "site 132 out of 158 is completed\n",
      "site 133 out of 158 is completed\n",
      "site 134 out of 158 is completed\n",
      "site 135 out of 158 is completed\n",
      "site 136 out of 158 is completed\n",
      "site 137 out of 158 is completed\n",
      "site 138 out of 158 is completed\n",
      "site 139 out of 158 is completed\n",
      "site 140 out of 158 is completed\n",
      "site 141 out of 158 is completed\n",
      "site 142 out of 158 is completed\n",
      "site 143 out of 158 is completed\n",
      "site 144 out of 158 is completed\n",
      "site 145 out of 158 is completed\n",
      "site 146 out of 158 is completed\n",
      "site 147 out of 158 is completed\n",
      "site 148 out of 158 is completed\n",
      "site 149 out of 158 is completed\n",
      "site 150 out of 158 is completed\n",
      "site 151 out of 158 is completed\n",
      "site 152 out of 158 is completed\n",
      "site 153 out of 158 is completed\n",
      "site 154 out of 158 is completed\n",
      "site 155 out of 158 is completed\n",
      "site 156 out of 158 is completed\n",
      "site 157 out of 158 is completed\n"
     ]
    }
   ],
   "source": [
    "request = requests.get(MYANMMAR_CHARITIES_URL)\n",
    "soup = BeautifulSoup(request.text, 'html.parser')\n",
    "\n",
    "#find all the website links under the different sectors and geography\n",
    "charities_container_sectors = soup.find_all(\"td\", {'class':\"sector1\"})\n",
    "charities_container_geography = soup.find_all('td', {'class':'state1'})\n",
    "\n",
    "#generate the list of webcontainers\n",
    "list_of_website_container = gen_websitecontainer_to_scrap(charities_container_sectors)+gen_websitecontainer_to_scrap(charities_container_geography)\n",
    "\n",
    "#generate the list of websites\n",
    "list_of_website = gen_website_to_scrape(list_of_website_container)\n",
    "\n",
    "#deduplicate identical websites\n",
    "list_of_website = list(set(list_of_website))\n",
    "\n",
    "#scrape all websites\n",
    "df = scrape_all(list_of_website)\n",
    "\n",
    "#clean and save output\n",
    "df = data_clean(df)\n",
    "\n",
    "df.to_csv(MYANMMAR_CSV_DUMP_PATH,na_rep='Nan',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name                0\n",
       "description       132\n",
       "website           115\n",
       "cause_area         14\n",
       "address           131\n",
       "city               36\n",
       "Contact_number     49\n",
       "email              93\n",
       "contact_person     78\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
